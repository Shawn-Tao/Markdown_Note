# SLAM14讲 后端1

## 贝叶斯公式

贝叶斯公式最简单表述形式如下
$$
\begin{aligned}
    P(A \mid B) = \frac{P(A \Cap B)}{P(B)} \\
    P(A \mid B) P(B) = P(A \Cap B) = P(B \mid A) P(A) \\
    P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}
    \tag{9.1}
\end{aligned}
$$
利用此公式可推广多条件情况
$$
P(A \mid B,C) = \frac{P(A)P(B \mid A) P(C \mid A ,b)}{P(C \mid B) P(B)}
\tag{9.2}
$$
多条件情况有多种表述方式

## 高斯分布定义

### 一维标准高斯分布

$$
P(x) = \frac{1}{\sqrt{2 \pi}} \exp (-\frac{x^2}{2})
$$

### 一维一般高斯分布

引入线性变换 $x:=A(x−μ),\:\sigma = 1/A$
$$
P(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp (-\frac{(x-\mu)^2}{2\sigma^2})
$$

### 二维高斯分布(各变量独立)

$P(x,y) = P(x) P(y)$ 定义 $\overrightarrow{v} = [x,y]^T$, 有:
$$
P(\overrightarrow{v}) = \frac{1}{2\pi} \exp(-\frac{1}{2}) \overrightarrow{v^T} \overrightarrow{v}
$$
令 $\overrightarrow{v} = A(\overrightarrow{x} - \mu)$ 得到一般形式
$$
P(\overrightarrow{v}) = \frac{A}{2 \pi} \exp(-\frac{1}{2}(x-\mu)^T A^T A (x-\mu))
$$
其中，定义 协方差矩阵 $\Sigma = A^T A$

### 高维高斯分布

$$
P(v) = \frac{1}{(2\pi)^{\frac{n}{2}} \mid \Sigma \mid^\frac{1}{2}} \exp(-\frac{1}{2} (v-\mu)^T \Sigma^{-1} (v-\mu))\:\: \overrightarrow{v} \in R^n
$$

## 协方差矩阵

方差用来度量随机变量的离散程度
协方差用来刻画两个随机变量相似程度

### 方差

$$
\sigma^2_x = \frac{1}{n-1} \sum^n_{i=1} (x_i-\overline{x})^2
$$

### 协方差定义

$$
\sigma(x,y) = \frac{1}{n-1} \sum^n_{i=1} (x_i - \overline{x})(y_i - \overline{y})
$$

如上，方差可以视为随机变量关于其自身的协方差

### 协方差矩阵引出

对于给定 $d$ 个随机变量 $x_k,k = 1,2...,d$, 这些随机变量方差为
$$
\sigma(x_k,x_k) = \frac{1}{n-1} \sum^n_{i=1} (x_{ki}-\overline{x}_k)^2\: \: k = 1,2,...,d
$$
而两两之间的协方差为
$$
\sigma(x_m,x_k) = \frac{1}{n-1} \sum^n_{i=1} (x_{mi} - \overline{x}_m)(x_{ki} - \overline{x}_k)
$$
从而得到协方差矩阵
$$
\Sigma = \left[
    \begin{matrix}
        \sigma(x_1,x_1) & \cdots & \sigma(x_1,x_d) \\
        \vdots & \ddots & \vdots \\
        \sigma(x_d,x_1) & \cdots & \sigma(x_d,x_d)
    \end{matrix}
    \right] \in R^{d \times d}
$$
<font color = #FF6197 size = 3>显然,协方差矩阵 $\Sigma$ 为对称矩阵,其大小为 $d\times d$ </font>

### 多元正态分布于线性变换

对于各个变量以零为均值的标准正态分布
$$
P(v) = \frac{1}{(2\pi)^{\frac{n}{2}} \mid \Sigma \mid^\frac{1}{2}} \exp(-\frac{1}{2} v^T \Sigma^{-1} v)\:\: \overrightarrow{v} \in R^n
$$
随机生成若干个点,每个点似然为
$$
\mathcal{L} (v) \propto \exp(-\frac{1}{2} v^T v)
$$
,随后对所有点做线性变换
$$
t = A v
$$
其中 $A$ 被称为变换矩阵,它由两个矩阵构成  
尺度矩阵
$$
\left[
\begin{matrix}
    s_y & 0 \\
    0 & s_z
\end{matrix}
\right]
$$
旋转矩阵
$$
\left[
\begin{matrix}
    \cos(\theta) & -\sin(\theta) \\
    \sin(\theta) & \cos(\theta)
\end{matrix}
\right]
$$
其中 $\theta$ 为顺时针旋转的度数 且 $A = RS$  
将$t = A v$ 带入前面给出的似然 ,有
$$
\mathcal{L} (t) \propto \exp(-\frac{1}{2} (A v)^T (A v) ) = \exp(-\frac{1}{2}  t^T (A^T A) t)
$$
由此可以得到,多元正态分布的协方差矩阵为
$$
\Sigma = A^T A
$$

## 状态估计概率解释

运动与观测方程描述SLAM运动，如下
$$
x_k = f(x_{k-1}，u_k) + \omega_k
z_{k,j} = h(y_i,x_k) + v_{k,j}
$$
实际中观测方程数量可能远多于运动方程，也可能没有运动方程  
批量状态估计问题可以转化为最大似然估计问题，并使用最小二乘法进行求解  
考虑第 $k$ 时刻情况
$$
P(x_k\mid x_0,u_{1:k},z_{1:k})
$$
按照贝叶斯法则,可以得到
$$
P(x_k\mid x_0,u_{1:k},z_{1:k}) \propto P(z_k\mid x_k) P(x_k\mid x_0,u_{1:k},z_{1:k-1})
$$
此处第一项称为似然,第二项称为先验,假设马尔科夫性,将 先验部分 展开并简化得到
$$
P(x_k\mid x_0,u_{1:k},z_{1:k-1}) = \int P(x_k\mid x_{k-1},u_k) P(x_{k-1}\mid x_0,u_{1:k-1},z_{1:k-1})
$$
则得到
$$
\begin{aligned}
    P(x_k\mid x_0,u_{1:k},z_{1:k}) \\
    P(x_{k-1}\mid x_0,u_{1:k-1},z_{1:k-1})
\end{aligned}
$$
两者之间关系,如此看来,我们实际上实在将 $k-1$ 时刻状态推导至 $k$ 时刻

## 线性高斯系统推导

线性高斯系统指,运动和观测方程可以由线性方程描述:
$$
    \left\{
        \begin{aligned}
        &x_k = A_k x_{k-1} + u_k + \omega_k \\
        &z_k = C_k x_k + v_k
        \end{aligned}
    \right.
$$
显然有
$$
P_(x_k\mid x_0, u_{1:k}, z_{1:k-1}) = N(A_k\hat{x}_{k-1}+u_k,A_k \hat{P}_{k-1} A^T_k + R)
$$
这一步称为预测(通过输入信息确定当前状态分布,这个分布就是先验):
$$
\begin{aligned}
    &\check{x}_k = A_k \hat{x}_{k-1} + u_k \\
    &\check{P}_k = A_k \check{P}_{k-1}A_k^T + R
\end{aligned}
$$
另一方面,通过观测方程我们可以计算在某个状态下应该产生怎样的观测数据
$$
P(z_k\mid x_k) = N(C_k x_k,Q)
$$
将结果设为
$$
x_k \sim N(\hat{x}_k,\hat{P}_k)
$$
那么:
$$
N(\hat{x}_k,\hat{P}_k) = \N(C_k x_k,Q) \times N(\check{x}_k, \check{P}_k)
$$
此处推导,我们仅对指数部分进行比较,较为简单,对于二次系数
$$
\hat{P}_k^{-1} = C_k^T Q^{-1} C_k + \check{P}_k^{-1}
$$
定义中间变量
$$
K = \hat{P}_k C_k^T Q^{-1}
$$
化简得到
$$
\hat{P}_k = (I-KC_k)\check{P}_k
$$
比较一次项的系数
$$
-2\hat{x}_k^T \hat{P}_k^{-1}x_k = -2z_k^T Q^{-1} C_k x_k - 2\check{x}_k^T \check{P}_k^{-1}x_k
$$
整理得到
$$
\hat{x}_k = \check{x}_k + K (z_k - C_k \check{x}_k)
$$

整理上述步骤得到经典卡尔曼滤波过程(最大后验估计方式)

一.预测
$$
\check{x} = A \hat{x}_{k-1} + u_k, \check{P}_k = A_k \hat{P}_{k-1}A_k^T + R
$$
二.更新：先计算K,它又称为卡尔曼增益
$$
K = \check{P}_k C_k^T (C_k \check{P}_k C_K^T + Q_k)^{-1}
$$
然后计算后验概率分布
$$
\begin{aligned}
   & \hat{x}_k = \check{x}_k + K (z_k - C_k \check{x}_k)\\
   & \hat{P}_k = (I-KC_k)\check{P}_k
\end{aligned}
$$

## 非线性系统与EKF

实际中，SLAM运动方程和观测方程通常是非线性函数，相机模型亦是非线性系统  
因此需要取近似，将非高斯分布近似成高斯分布  
卡尔曼滤波器拓展到非线性系统中，称为拓展卡尔曼滤波，通产做法是，将某个点附近考虑运动方程及观测方程的一阶泰勒展开，只保留一阶项(线性部分)  

