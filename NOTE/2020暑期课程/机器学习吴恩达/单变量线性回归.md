# 单变量线性回归(linear regression with one variable)

## hypothesis function

训练集 $\rightarrow$ 学习算法 $\rightarrow$ (假设函数)  
输入 $\rightarrow$ 假设函数 $\rightarrow$ 输入  
单变量情况 h = $\theta_0 + \theta_1 x$  

## 模型实现

Hypothesis:$h_{\theta}(x) = \theta_0 + \theta_1 x$  
$\theta_{i's}$ : parameters  (How to choose)  
建模为：
$$
\min_{\theta_0 \theta_1} \frac{1}{2m} \sum^m_{i=1} (h_{\theta}(x^{(i)}) -y^{ (i)})^2
$$
改写为代价函数(cost function)----平方误差函数,对于regression函数较为使用 
$$
\begin{aligned}
   & J(\theta_0,\theta_1) = \frac{1}{2m} \sum^m_{i=1} (h_{\theta}(x^{(i)}) \\
   & \min_{\theta_0,\theta_1} J(\theta_0,\theta_1)
\end{aligned}
$$

## 梯度下降法(gradient descent)

Function : $j(\theta_0,\theta_1)$  
Want : $\min_{\theta_0,\theta_1} J(\theta_0,\theta_1)$  
outline:

1. Start with random $\theta_0,\theta_1$
2. Keep changint $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$ until we hopefully end up at a minimum

repeat until comvergence {  
    &emsp; $\theta_j = \theta_j -\alpha \frac{\partial J(\theta_0,\theta_1)}{\partial \theta_j}$ for ($j == 0$ and $j == 1$)  
}  

$\alpha$ is called Learning Rate  
The correct implementation of gradient descent: simultaneous updates:  

temp0 = $\theta_0-\alpha \frac{\partial J(\theta_0,\theta_1)}{\partial \theta_0}$  
temp1 = $\theta_1-\alpha \frac{\partial J(\theta_0,\theta_1)}{\partial \theta_1}$  
$\theta_0$ = temp0  
$\theta_1$ = temp1  

在逐渐接近最低点过程中，采用偏导数，可以逐渐降低沿梯度方向下降的幅度，从而提高收敛的可能性  

Convex function 只有全局最优接，没有局部最优  

Batch Gradient Descent